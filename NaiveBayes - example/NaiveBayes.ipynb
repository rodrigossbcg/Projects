{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 20px;\">\n",
    " \n",
    "<center>    \n",
    "    <h1><b>Classification Model - NaiveBayes</b></h1>\n",
    "    <h2>Iris dataset</h2>\n",
    "</center>\n",
    "    \n",
    "<br>\n",
    "<br>    \n",
    "    \n",
    "<div style=\"background-color: white; padding: 20px; border-radius: 10px; width: 70%; margin-left: 15%;\">    \n",
    "    <img src=\"iris.png\">    \n",
    "</div>  \n",
    "    \n",
    "<br>    \n",
    "    <div style=\"background-color: white; color: #777877; width: 30%; margin-left: 35%; padding: 20px; font-size: 16px; line-height: 25px; border-radius: 10px;\">\n",
    "        <ul>\n",
    "            <li>0. Introduction</li>\n",
    "            <li>1. Import Dataset</li>\n",
    "            <li>2. Analisys</li>\n",
    "            <li>3. Visualization</li>\n",
    "            <li>4. Modeling</li>\n",
    "            <li>5. Cross Validation</li>\n",
    "            <li>6. Prediction Metrics</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "<br>\n",
    "    \n",
    "<b>By:</b> Rodrigo Sarroeira    \n",
    "<br>    \n",
    "<b>On:</b> 22/02/2021    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 20px;\"> \n",
    "    <h2 id=\"intro\"><b>0. Introduction </b></h2>\n",
    "</div>\n",
    "    \n",
    "<p style=\"text-align: justify; padding: 15px;\">\n",
    "In this notebook we are going to implement a very simple example of the Naive Bayes algorithm. The dataset in use will be the famous Iris, used many times for classification demonstrations. This dataset has information about 3 Species of Iris (Setosa, Versicolour, Virginica). The main goal is for our model to read the data and identify each observation correctly, as Setosa, Versicolour or Virginica.\n",
    "    \n",
    "<br>\n",
    "    \n",
    "<p style=\"text-align: justify; padding: 15px;\">The <b>Naive Bayes</b> algorithm is based on a very simple probabilistic concept, called conditional probability. The Bayes Theorem calculates the probability of a certain event given some pior knowledge related to the event. This Theorem is of great importance for some algorithms, but it is also important to understand better the probabilities of events, based on other factors. </p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "<div style=\"background-color: #dedede; padding: 30px; border-radius: 10px;\">\n",
    "    \n",
    "<center><h2><b style=\"color: white;\">Bayes Formula</b></h2></center>\n",
    "    \n",
    "<div style=\"background-color: white; padding: 20px; border-radius: 10px; width: 40%; margin-left: 30%; margin-top: 15px;\">\n",
    "\n",
    "$$\n",
    "    \\ P(A|B) = P(A) \\ * \\ \\frac{P(B|A)} {P(B)}  \n",
    "$$\n",
    "    \n",
    "</div>\n",
    "</div>    \n",
    "\n",
    "<br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(naivebayes)  # Naive Bayes\n",
    "library(ggplot2)     # Visualization\n",
    "library(dplyr)       \n",
    "library(psych)       # Graph   \n",
    "library(caret)       # Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 20px;\"> \n",
    "<h2>1. Import Dataset</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data(iris)\n",
    "head(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 20px;\"> \n",
    "<h2>2. Analisys</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is composed 150 observations and by 5 variables, from wich 4 are numeric and 1 is a factor. The `Species` variable is the <b style=\"color: red;\">target</b> variable, or in other words, is the variable we are trying to predict based on the other 4 variables. The independent variables represent widths and lenghts of petals and sepals. Our <b style=\"color: red;\">target</b> variable has 3 possible values: setosa, versicolor and virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 20px;\"> \n",
    "<h3>2.1 Visualization</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of correlation between numeric variables\n",
    "pairs.panels(iris[-5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at this graph we are given a lot of information. The distribution of each variable can be found on the principal diagonal. `Sepal.Length` and `Sepal.Width` distribution's are almost simetric, on the other hand, on the ditribution graphs of `Petal.Length` and `Petal.Width` we can find two \"clusters\". This means that the variables related to the Petals are going to be good predictors of the target variable. \n",
    "<br>\n",
    "In addiction this graph gives us the linear correlation between all variables. The correlation between `Petal.Width` and `Petal.Length` is very high,<b> 0.96</b>. There are 3 positive correlations and 3 negative correlations. The lowest correlation is between `Sepal.Length` and `Sepal.Width`, with a value of <b>-0.12</b>.\n",
    "<br>\n",
    "On the lower triangle of this matrix we can see how the variables relate to each other, by interpreting the scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(iris, aes(x=Species, y=Petal.Width, fill=Species)) +\n",
    "    geom_boxplot() + \n",
    "    ggtitle(\"Boxplot Petal Width by Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scatter plots built on `ggplot` are a very usefull tool when the target variable is categorical. In each graph we have the information about how the variable is spread, given the `Species`. For example, on the last graph we see that `Setosas` have the smallest Petal Width and that the `Virginicas` have the largest Petals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(iris, aes(x=Species, y=Sepal.Width, fill=Species)) +\n",
    "    geom_boxplot() + \n",
    "    ggtitle(\"Boxplot Sepal Width by Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Boxplot Sepal Width by Species\" shows us that the `Sepal.Width` isnÂ´t a very good variable to use for the classification, because it's values for different Species are very identical. The next two graphs will reenforce this idea. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(iris, aes(x=Petal.Width, fill=Species)) +\n",
    "    geom_density() + \n",
    "    ggtitle(\"Density plot for Petal Width by Species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(iris, aes(x=Sepal.Width, fill=Species)) +\n",
    "    geom_density() + \n",
    "    ggtitle(\"Density plot for Sepal Width by Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first graph clearly shows that the `Petal.Width` is a good variable to use in the classification formula, because each of the Species have very different values for the Petal Width. On the other hand, when we look to the density graph of the variable `Sepal.Width` we see that all the species share, more or less, the same values for the Sepal Width, making it hard to distinguish them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 20px;\"> \n",
    "<h2>3. Modeling </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating a final model, I will create two example models. One using `Petal.Width` and another using `Sepal.Width`. The goal of this examples is to demonstrate how this two variables preform very differently when used on simple naive bayes models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 20px;\"> \n",
    "<h3>3.1 Modeling with Petal Width </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb1 = naive_bayes(Species ~ Petal.Width, data=iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "predicted1 = predict(nb1, iris)\n",
    "tb1 = table(predicted1, iris$Species)\n",
    "tb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missclassified values\n",
    "1 - sum(diag(tb1)) / sum(tb1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only by analizing the boxplot and the density plot for this variable, we were expecting a low percentage of missclassified values. The values of this variable are grouped on clusters, one for each `Species`, making it very easy for the algorithm to predict the right classification. We have a 100% accuracy on `Setosas` and there were only 6 missclassified values, from wich 4 were `Virginicas` and the other 2 `Versicolor`. This represent a total accuracy of<b> 0.96</b>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 20px;\"> \n",
    "<h3>3.2 Modeling with Sepal Width </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb2 = naive_bayes(Species ~ Sepal.Width, data=iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted2 = predict(nb2, iris)\n",
    "tb2 = table(predicted2, iris$Species)\n",
    "tb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missclassified values\n",
    "1 - sum(diag(tb2)) / sum(tb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the plots for `Sepal.Width` we understand that all Speaces have similar Sepal widths, making it harder for the algorithm to predict the class correctly. Almost half of the observations were classified correctly, and when we look at the classification of virginicas the number of missclassified values is higher than half (27 ou of 50). In this model the Setosas are the Species with more accurately predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 20px;\"> \n",
    "<h3>3.3 Final Model </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with all the vairiables\n",
    "nb_final = naive_bayes(Species ~ Petal.Width * Petal.Length + Sepal.Length, data=iris)\n",
    "\n",
    "# by adding Sepal.Width to the model the number of missclassified values is incremented by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creationg of the confusion matrix\n",
    "predicted_final = predict(nb_final, iris)\n",
    "tb = table(predicted_final, iris$Species)\n",
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missclassified values\n",
    "1 - sum(diag(tb)) / sum(tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has the best performance of all possible combinations between all the available variables. The accuracy of this model is <b>96,66%</b>. To make sure our model preforms well for new data, we are going to use a method called <b>Cross Validation</b>, this method enables predicting data from the \"train set\", as if it is new data for the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 20px;\"> \n",
    "<h2>4. Cross validation - using 6 folds</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of observation is low (150 is very low), it is usual to resort to the K-fold method. This method divides the dataset in K folds, using K - 1 folds to train the model and 1 folds to test the model. This process is repeated K times, as result all the observations will be predicted as they were new data to the model. This way we garentee that the predictions we are making are not only good `in-sample`, but also `out-of-sample`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6   # number of folds\n",
    "folds = createFolds(iris$Species, k, list=TRUE, returnTrain=FALSE)\n",
    "str(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating vector to store predicted values\n",
    "predict_vector = rep(NA, nrow(iris))\n",
    "\n",
    "\n",
    "for(i in 1:k){ \n",
    "    \n",
    "    cross_model = naive_bayes(Species ~ Petal.Width + Petal.Length + Sepal.Length, data=iris[-folds[[i]],])\n",
    "    test_data = iris[folds[[i]] , c(1,2,3,4)]   \n",
    "    predict_vector[folds[[i]]] = predict(cross_model, test_data)\n",
    "\n",
    "}\n",
    "\n",
    "predict_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output vector classifies each `Specie` with a number, to enable the comparison between `predicted_vector` and the actual values, we must replace all numbers with the respective `Species` associated with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted values for Species\n",
    "predict_vector[predict_vector==1] = \"setosa\"\n",
    "predict_vector[predict_vector==2] = \"versicolor\"\n",
    "predict_vector[predict_vector==3] = \"virginica\"\n",
    "\n",
    "predict_vector = as.factor(predict_vector)\n",
    "\n",
    "predict_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_final = table(predict_vector, iris$Species)\n",
    "tb_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = 1 - sum(diag(tb_final)) / sum(tb_final)\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we made sure that our model preforms well, both <b>in</b> and <b>out</b> of sample. Using this method we got a very low error of classification, only <b>4% or 5%</b> of the values were not label correctly. The `performance` can have different values, because the method that the function `createFolds()` uses has a random component, making it possible for values to float round 4 or 5 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 20px;\"> \n",
    "<h2>5. Prediction Metrics </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the preformance of our model we are going to use come metrics. To calculate them we are going to use the values of the confusion matrix created before, stored inside the `tb_final` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `accuracy` is a metric that calculates the proportion of well classified observations. Therefore you can obtain the proportion of missclassified observations by doing `1 - accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of certain predictions\n",
    "accuracy = sum(diag(tb_final)) / sum(tb_final)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of missclassified values\n",
    "missclassified = 1 - sum(diag(tb_final)) / sum(tb_final)\n",
    "missclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision for `Setosas` evaluates the proportion of real Setosas out of all the predicted Setosas. This metric gives us the level of certainty, with which we can predict observations of a determined category. As we can see the `precision_setosa` is `1`, meaning that all setosas were well classified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision prediction setosas\n",
    "precision_setosa = tb_final[1,1] / sum(tb_final[,1])\n",
    "precision_setosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision prediction versicolors\n",
    "precision_versicolor = tb_final[2,2] / sum(tb_final[,2])\n",
    "precision_versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision prediction virginic\n",
    "precision_virginica = tb_final[3,3] / sum(tb_final[,3])\n",
    "precision_virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the sensivity metric measures the proportion of correcly identified observations, out of all real observations of that category. For example, as `sensivity_versicolor = 1`, the proportion of well classified `versicolors` out of all `versicolors` is 94,12%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of right classifications for setosas\n",
    "sensivity_setosa = tb_final[1,1] / sum(tb_final[1,])\n",
    "sensivity_setosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of right classifications for versicolors\n",
    "sensivity_versicolor = tb_final[2,2] / sum(tb_final[2,])\n",
    "sensivity_versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of right classifications for virginicas\n",
    "sensivity_virginica = tb_final[3,3] / sum(tb_final[3,])\n",
    "sensivity_virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white;\">\n",
    "<br>\n",
    "<br>\n",
    "<center><h2><b> We have reached the end of this simple example. Thanks for reading!  </b></h2></center>\n",
    "<br>\n",
    "<br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
